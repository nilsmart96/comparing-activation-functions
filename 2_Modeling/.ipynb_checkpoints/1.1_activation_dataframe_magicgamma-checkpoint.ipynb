{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from livelossplot import PlotLossesKeras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "# set random seed globally\n",
    "my_seed = 21\n",
    "from numpy.random import seed\n",
    "seed(my_seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(my_seed) \n",
    "# tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAGIC Gamma Telescope Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19015</th>\n",
       "      <td>21.3846</td>\n",
       "      <td>10.9170</td>\n",
       "      <td>2.6161</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.3934</td>\n",
       "      <td>15.2618</td>\n",
       "      <td>11.5245</td>\n",
       "      <td>2.8766</td>\n",
       "      <td>2.4229</td>\n",
       "      <td>106.8258</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19016</th>\n",
       "      <td>28.9452</td>\n",
       "      <td>6.7020</td>\n",
       "      <td>2.2672</td>\n",
       "      <td>0.5351</td>\n",
       "      <td>0.2784</td>\n",
       "      <td>37.0816</td>\n",
       "      <td>13.1853</td>\n",
       "      <td>-2.9632</td>\n",
       "      <td>86.7975</td>\n",
       "      <td>247.4560</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19017</th>\n",
       "      <td>75.4455</td>\n",
       "      <td>47.5305</td>\n",
       "      <td>3.4483</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>-9.3561</td>\n",
       "      <td>41.0562</td>\n",
       "      <td>-9.4662</td>\n",
       "      <td>30.2987</td>\n",
       "      <td>256.5166</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19018</th>\n",
       "      <td>120.5135</td>\n",
       "      <td>76.9018</td>\n",
       "      <td>3.9939</td>\n",
       "      <td>0.0944</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>5.8043</td>\n",
       "      <td>-93.5224</td>\n",
       "      <td>-63.8389</td>\n",
       "      <td>84.6874</td>\n",
       "      <td>408.3166</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19019</th>\n",
       "      <td>187.1814</td>\n",
       "      <td>53.0014</td>\n",
       "      <td>3.2093</td>\n",
       "      <td>0.2876</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>-167.3125</td>\n",
       "      <td>-168.4558</td>\n",
       "      <td>31.4755</td>\n",
       "      <td>52.7310</td>\n",
       "      <td>272.3174</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19020 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fLength    fWidth   fSize   fConc  fConc1     fAsym   fM3Long  \\\n",
       "0       28.7967   16.0021  2.6449  0.3918  0.1982   27.7004   22.0110   \n",
       "1       31.6036   11.7235  2.5185  0.5303  0.3773   26.2722   23.8238   \n",
       "2      162.0520  136.0310  4.0612  0.0374  0.0187  116.7410  -64.8580   \n",
       "3       23.8172    9.5728  2.3385  0.6147  0.3922   27.2107   -6.4633   \n",
       "4       75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277   28.5525   \n",
       "...         ...       ...     ...     ...     ...       ...       ...   \n",
       "19015   21.3846   10.9170  2.6161  0.5857  0.3934   15.2618   11.5245   \n",
       "19016   28.9452    6.7020  2.2672  0.5351  0.2784   37.0816   13.1853   \n",
       "19017   75.4455   47.5305  3.4483  0.1417  0.0549   -9.3561   41.0562   \n",
       "19018  120.5135   76.9018  3.9939  0.0944  0.0683    5.8043  -93.5224   \n",
       "19019  187.1814   53.0014  3.2093  0.2876  0.1539 -167.3125 -168.4558   \n",
       "\n",
       "       fM3Trans   fAlpha     fDist class  \n",
       "0       -8.2027  40.0920   81.8828     g  \n",
       "1       -9.9574   6.3609  205.2610     g  \n",
       "2      -45.2160  76.9600  256.7880     g  \n",
       "3       -7.1513  10.4490  116.7370     g  \n",
       "4       21.8393   4.6480  356.4620     g  \n",
       "...         ...      ...       ...   ...  \n",
       "19015    2.8766   2.4229  106.8258     h  \n",
       "19016   -2.9632  86.7975  247.4560     h  \n",
       "19017   -9.4662  30.2987  256.5166     h  \n",
       "19018  -63.8389  84.6874  408.3166     h  \n",
       "19019   31.4755  52.7310  272.3174     h  \n",
       "\n",
       "[19020 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataframe from https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data',\n",
    "                 names = ['fLength','fWidth','fSize','fConc','fConc1','fAsym','fM3Long','fM3Trans','fAlpha','fDist','class'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet (dataframe, test_data_fraction):\n",
    "    # encoding the target column\n",
    "    le = LabelEncoder()\n",
    "    label = le.fit_transform(df['class'])\n",
    "    label\n",
    "\n",
    "    encoded_df = df.copy()\n",
    "    encoded_df.drop(\"class\", axis=1, inplace=True)\n",
    "    encoded_df[\"class\"] = label\n",
    "\n",
    "    # Set the total number of classes\n",
    "    nb_classes = len(encoded_df['class'].unique())\n",
    "\n",
    "    # Creating target and features\n",
    "    X = encoded_df.drop(['class'], axis=1)\n",
    "    y = encoded_df['class']\n",
    "\n",
    "    # scale the variables\n",
    "    sc = StandardScaler() \n",
    "    X_scaled = sc.fit_transform(X)\n",
    "\n",
    "    # Split into train and test set and normalize data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size = test_data_fraction,stratify=y) #, random_state = 0)\n",
    "\n",
    "\n",
    "    return encoded_df, nb_classes, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "class CustomWeight(tf.keras.initializers.Initializer):\n",
    "    def __init__(self):\n",
    "        print('gg')\n",
    "    def constant_weight(self,num):\n",
    "        return tf.keras.initializers.Constant(num)\n",
    "    \n",
    "    def normal_weight(self,mean,stddev):\n",
    "        return tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "\n",
    "\n",
    "def buildSequentialModel(hidden_layers_activation, first_layer_node_count, dropout_fraction, nb_classes,weight_class, wt_mean, wt_std, bias_mean, bias_std):\n",
    "    # Build a Sequential Model.\n",
    "    model = Sequential()\n",
    "    # model.add(Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    model.add(Dense(first_layer_node_count, kernel_initializer=weight_class.normal_weight(wt_mean,wt_std), activation=hidden_layers_activation,bias_initializer=weight_class.normal_weight(bias_mean,bias_std)))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "    model.add(Dense(first_layer_node_count*0.8, kernel_initializer=weight_class.normal_weight(wt_mean,wt_std), activation=hidden_layers_activation,bias_initializer=weight_class.normal_weight(bias_mean,bias_std)))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "    model.add(Dense(first_layer_node_count*0.6, kernel_initializer=weight_class.normal_weight(wt_mean,wt_std), activation=hidden_layers_activation,bias_initializer=weight_class.normal_weight(bias_mean,bias_std)))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(nb_classes, activation=hidden_layers_activation, kernel_initializer=weight_class.normal_weight(wt_mean,wt_std), bias_initializer=weight_class.normal_weight(bias_mean,bias_std)))\n",
    "   \n",
    "   \n",
    "    return model\n",
    "\n",
    "\n",
    "''' def reset_weights(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.Model): #if you're using a model as a layer\n",
    "            reset_weights(layer) #apply function recursively\n",
    "            continue\n",
    "\n",
    "        #where are the initializers?\n",
    "        if hasattr(layer, 'cell'):\n",
    "            init_container = layer.cell\n",
    "        else:\n",
    "            init_container = layer\n",
    "\n",
    "        for key, initializer in init_container.__dict__.items():\n",
    "            if \"initializer\" not in key: #is this item an initializer?\n",
    "                  continue #if no, skip it\n",
    "\n",
    "            # find the corresponding variable, like the kernel or the bias\n",
    "            if key == 'recurrent_initializer': #special case check\n",
    "                var = getattr(init_container, 'recurrent_kernel')\n",
    "            else:\n",
    "                var = getattr(init_container, key.replace(\"_initializer\", \"\"))\n",
    "\n",
    "            var.assign(initializer(var.shape, var.dtype))\n",
    "            #use the initializer '''\n",
    "\n",
    "\n",
    "def Study_Activation_Functions(hidden_layers_activation, first_layer_node_count, dropout_fraction, classes, X_train, X_test, y_train, y_test,weight_class, wt_mean, wt_std, bias_mean, bias_std):\n",
    "    \n",
    "    # instantiate model\n",
    "    model = buildSequentialModel(hidden_layers_activation, first_layer_node_count, dropout_fraction, classes,weight_class,  wt_mean, wt_std, bias_mean, bias_std)\n",
    "    # compile model\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "    train_score = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "    test_score = model.evaluate(X_test, y_test, verbose=0)[1] \n",
    "\n",
    "    predicted_classes = np.argmax(model.predict(X_test), axis=-1)\n",
    "    correct_indices = np.nonzero(predicted_classes == y_test.values)[0]\n",
    "    incorrect_indices = np.nonzero(predicted_classes != y_test.values)[0]\n",
    "\n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataframe = (19020, 11)\n",
      "classes in dataset = 2\n",
      "X_train.shape = (15216, 10)\n",
      "X_test.shape = (3804, 10)\n",
      "y_train.shape = (15216,)\n",
      "y_test.shape = (3804,)\n",
      "\n",
      "encoded_dataframe shown below:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19015</th>\n",
       "      <td>21.3846</td>\n",
       "      <td>10.9170</td>\n",
       "      <td>2.6161</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>0.3934</td>\n",
       "      <td>15.2618</td>\n",
       "      <td>11.5245</td>\n",
       "      <td>2.8766</td>\n",
       "      <td>2.4229</td>\n",
       "      <td>106.8258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19016</th>\n",
       "      <td>28.9452</td>\n",
       "      <td>6.7020</td>\n",
       "      <td>2.2672</td>\n",
       "      <td>0.5351</td>\n",
       "      <td>0.2784</td>\n",
       "      <td>37.0816</td>\n",
       "      <td>13.1853</td>\n",
       "      <td>-2.9632</td>\n",
       "      <td>86.7975</td>\n",
       "      <td>247.4560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19017</th>\n",
       "      <td>75.4455</td>\n",
       "      <td>47.5305</td>\n",
       "      <td>3.4483</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>-9.3561</td>\n",
       "      <td>41.0562</td>\n",
       "      <td>-9.4662</td>\n",
       "      <td>30.2987</td>\n",
       "      <td>256.5166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19018</th>\n",
       "      <td>120.5135</td>\n",
       "      <td>76.9018</td>\n",
       "      <td>3.9939</td>\n",
       "      <td>0.0944</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>5.8043</td>\n",
       "      <td>-93.5224</td>\n",
       "      <td>-63.8389</td>\n",
       "      <td>84.6874</td>\n",
       "      <td>408.3166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19019</th>\n",
       "      <td>187.1814</td>\n",
       "      <td>53.0014</td>\n",
       "      <td>3.2093</td>\n",
       "      <td>0.2876</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>-167.3125</td>\n",
       "      <td>-168.4558</td>\n",
       "      <td>31.4755</td>\n",
       "      <td>52.7310</td>\n",
       "      <td>272.3174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19020 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fLength    fWidth   fSize   fConc  fConc1     fAsym   fM3Long  \\\n",
       "0       28.7967   16.0021  2.6449  0.3918  0.1982   27.7004   22.0110   \n",
       "1       31.6036   11.7235  2.5185  0.5303  0.3773   26.2722   23.8238   \n",
       "2      162.0520  136.0310  4.0612  0.0374  0.0187  116.7410  -64.8580   \n",
       "3       23.8172    9.5728  2.3385  0.6147  0.3922   27.2107   -6.4633   \n",
       "4       75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277   28.5525   \n",
       "...         ...       ...     ...     ...     ...       ...       ...   \n",
       "19015   21.3846   10.9170  2.6161  0.5857  0.3934   15.2618   11.5245   \n",
       "19016   28.9452    6.7020  2.2672  0.5351  0.2784   37.0816   13.1853   \n",
       "19017   75.4455   47.5305  3.4483  0.1417  0.0549   -9.3561   41.0562   \n",
       "19018  120.5135   76.9018  3.9939  0.0944  0.0683    5.8043  -93.5224   \n",
       "19019  187.1814   53.0014  3.2093  0.2876  0.1539 -167.3125 -168.4558   \n",
       "\n",
       "       fM3Trans   fAlpha     fDist  class  \n",
       "0       -8.2027  40.0920   81.8828      0  \n",
       "1       -9.9574   6.3609  205.2610      0  \n",
       "2      -45.2160  76.9600  256.7880      0  \n",
       "3       -7.1513  10.4490  116.7370      0  \n",
       "4       21.8393   4.6480  356.4620      0  \n",
       "...         ...      ...       ...    ...  \n",
       "19015    2.8766   2.4229  106.8258      1  \n",
       "19016   -2.9632  86.7975  247.4560      1  \n",
       "19017   -9.4662  30.2987  256.5166      1  \n",
       "19018  -63.8389  84.6874  408.3166      1  \n",
       "19019   31.4755  52.7310  272.3174      1  \n",
       "\n",
       "[19020 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataset on our dataframe\n",
    "encoded_dataframe, classes, Train_FEATURE_matrix, Test_FEATURE_matrix, Train_TARGET_matrix, Test_TARGET_matrix = buildDataSet (df, 0.2)\n",
    "\n",
    "print(f\"shape of dataframe = {encoded_dataframe.shape}\")\n",
    "print(f\"classes in dataset = {classes}\")\n",
    "print(f\"X_train.shape = {Train_FEATURE_matrix.shape}\")\n",
    "print(f\"X_test.shape = {Test_FEATURE_matrix.shape}\")\n",
    "print(f\"y_train.shape = {Train_TARGET_matrix.shape}\")\n",
    "print(f\"y_test.shape = {Test_TARGET_matrix.shape}\")\n",
    "print(\"\")\n",
    "print(\"encoded_dataframe shown below:\")\n",
    "encoded_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conventional Activation Functions to test\n",
    "\n",
    "- Logistic regression hypothesis (Sigmoid)\n",
    "- Hyperbolic Tangent (tanh) # rescaled sigmoid to (-1, +1)\n",
    "- Rectified Linear Unit (ReLU)\n",
    "- Gaussian Error Linear Unit (GELU) # smoothed ReLU\n",
    "- Normalized Exponential Function (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gg\n",
      "Epoch 1/50\n",
      "143/143 [==============================] - 3s 5ms/step - loss: 4.1639 - accuracy: 0.5889 - val_loss: 4.5152 - val_accuracy: 0.6376\n",
      "Epoch 2/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 4.0542 - accuracy: 0.5999 - val_loss: 4.1868 - val_accuracy: 0.6374\n",
      "Epoch 3/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 4.0471 - accuracy: 0.6114 - val_loss: 3.7544 - val_accuracy: 0.6458\n",
      "Epoch 4/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 3.9676 - accuracy: 0.6076 - val_loss: 3.1584 - val_accuracy: 0.6593\n",
      "Epoch 5/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 3.8635 - accuracy: 0.6147 - val_loss: 2.5519 - val_accuracy: 0.6610\n",
      "Epoch 6/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 3.5245 - accuracy: 0.6196 - val_loss: 1.4027 - val_accuracy: 0.6593\n",
      "Epoch 7/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 3.3913 - accuracy: 0.6214 - val_loss: 1.3046 - val_accuracy: 0.6562\n",
      "Epoch 8/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 3.4448 - accuracy: 0.6148 - val_loss: 1.2210 - val_accuracy: 0.6555\n",
      "Epoch 9/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 3.3457 - accuracy: 0.6168 - val_loss: 1.1204 - val_accuracy: 0.6547\n",
      "Epoch 10/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 3.2272 - accuracy: 0.6151 - val_loss: 1.3132 - val_accuracy: 0.6454\n",
      "Epoch 11/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 3.0849 - accuracy: 0.6121 - val_loss: 1.0470 - val_accuracy: 0.6490\n",
      "Epoch 12/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 2.8837 - accuracy: 0.6153 - val_loss: 1.1828 - val_accuracy: 0.6456\n",
      "Epoch 13/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 2.6380 - accuracy: 0.6208 - val_loss: 0.7920 - val_accuracy: 0.6494\n",
      "Epoch 14/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 2.2275 - accuracy: 0.6320 - val_loss: 0.7010 - val_accuracy: 0.6484\n",
      "Epoch 15/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 1.7244 - accuracy: 0.6443 - val_loss: 0.7023 - val_accuracy: 0.6484\n",
      "Epoch 16/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 1.6618 - accuracy: 0.6437 - val_loss: 0.7024 - val_accuracy: 0.6484\n",
      "Epoch 17/50\n",
      "  1/143 [..............................] - ETA: 0s - loss: 2.0051 - accuracy: 0.6200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m haf \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(Hidden_AF_list)):\n\u001b[0;32m     22\u001b[0m     encoded_dataframe, classes, Train_FEATURE_matrix, Test_FEATURE_matrix, Train_TARGET_matrix, Test_TARGET_matrix \u001b[39m=\u001b[39m buildDataSet (df, TrainTest_Split)\n\u001b[1;32m---> 23\u001b[0m     train_accuracy_score, test_accuracy_score \u001b[39m=\u001b[39m Study_Activation_Functions(Hidden_AF_list[haf], First_Hidden_Layer_Tensor_Count, Dropouts, classes, Train_FEATURE_matrix, Test_FEATURE_matrix, Train_TARGET_matrix, Test_TARGET_matrix,CustomWeight(), wt_mean, wt_std, bias_mean, bias_std)\n\u001b[0;32m     24\u001b[0m     dataframes[df_index]\u001b[39m.\u001b[39mloc[i] \u001b[39m=\u001b[39m [encoded_dataframe\u001b[39m.\u001b[39mshape, classes, Hidden_AF_list[haf], TrainTest_Split, First_Hidden_Layer_Tensor_Count, Dropouts, np\u001b[39m.\u001b[39mround(train_accuracy_score,\u001b[39m3\u001b[39m), np\u001b[39m.\u001b[39mround(test_accuracy_score,\u001b[39m3\u001b[39m)]\n\u001b[0;32m     25\u001b[0m     i \u001b[39m=\u001b[39m i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[10], line 92\u001b[0m, in \u001b[0;36mStudy_Activation_Functions\u001b[1;34m(hidden_layers_activation, first_layer_node_count, dropout_fraction, classes, X_train, X_test, y_train, y_test, weight_class, wt_mean, wt_std, bias_mean, bias_std)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m# compile model\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m#model.compile(loss='categorical_crossentropy', optimizer='adam')\u001b[39;00m\n\u001b[0;32m     90\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m), loss\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 92\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test))\n\u001b[0;32m     94\u001b[0m train_score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_train, y_train, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     95\u001b[0m test_score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m1\u001b[39m] \n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 10 # number of iterations that we plan to perform for each pair of hidden_layer_af - output_layer_af\n",
    "dataframes = [pd.DataFrame() for _ in range(0, iterations)] # declare a list of empty dataframes to store the results of the iterations\n",
    "\n",
    "for df_index in range(0, iterations):\n",
    "    dataframes[df_index] = pd.DataFrame(columns = ['data_Dimension', 'Classes', 'Hidden_AF', 'TrainTest_Split', 'First_Hidden_Layer_Tensor_Count', 'Dropouts', 'Train_Accuracy', 'Test_Accuracy'])\n",
    "\n",
    "# results_df = pd.DataFrame(columns = ['data_Dimension', 'Classes', 'Hidden_AF', 'Output_AF', 'TrainTest_Split', 'First_Hidden_Layer_Tensor_Count', 'Dropouts', 'Train_Accuracy', 'Test_Accuracy'])\n",
    "\n",
    "Hidden_AF_list = ['relu', 'gelu', 'selu', 'sigmoid', 'tanh', 'softmax']\n",
    "# Output_AF_list = ['gelu', 'sigmoid']\n",
    "First_Hidden_Layer_Tensor_Count = 100\n",
    "TrainTest_Split = 0.25\n",
    "Dropouts = 0.3\n",
    "wt_mean=0\n",
    "wt_std=0.5\n",
    "bias_mean=0\n",
    "bias_std=0.25\n",
    "\n",
    "for df_index in range(0, iterations):\n",
    "    i = 0\n",
    "    for haf in range(0,len(Hidden_AF_list)):\n",
    "        encoded_dataframe, classes, Train_FEATURE_matrix, Test_FEATURE_matrix, Train_TARGET_matrix, Test_TARGET_matrix = buildDataSet (df, TrainTest_Split)\n",
    "        train_accuracy_score, test_accuracy_score = Study_Activation_Functions(Hidden_AF_list[haf], First_Hidden_Layer_Tensor_Count, Dropouts, classes, Train_FEATURE_matrix, Test_FEATURE_matrix, Train_TARGET_matrix, Test_TARGET_matrix,CustomWeight(), wt_mean, wt_std, bias_mean, bias_std)\n",
    "        dataframes[df_index].loc[i] = [encoded_dataframe.shape, classes, Hidden_AF_list[haf], TrainTest_Split, First_Hidden_Layer_Tensor_Count, Dropouts, np.round(train_accuracy_score,3), np.round(test_accuracy_score,3)]\n",
    "        i = i + 1\n",
    "    # model_.reset_states()\n",
    "    # reset_weights(model_)    \n",
    "     \n",
    "    # print(f\"dataframe {df_index + 1}:\")\n",
    "    # print(dataframes[df_index])\n",
    "    # print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = dataframes[0]\n",
    "results_df = results_df.rename(columns={\"Train_Accuracy\": \"Robust_Train_Accuracy\", \"Test_Accuracy\": \"Robust_Test_Accuracy\"})\n",
    "for df_index in range(1, iterations):\n",
    "    results_df['Robust_Train_Accuracy'] = results_df['Robust_Train_Accuracy'] + dataframes[df_index]['Train_Accuracy']\n",
    "    results_df['Robust_Test_Accuracy'] = results_df['Robust_Test_Accuracy'] + dataframes[df_index]['Test_Accuracy']\n",
    "results_df['Robust_Train_Accuracy'] = round((results_df['Robust_Train_Accuracy'] / iterations),4)\n",
    "results_df['Robust_Test_Accuracy'] = round((results_df['Robust_Test_Accuracy'] / iterations),4)\n",
    "\n",
    "results_df.to_csv(\"results_df.csv\")\n",
    "results_df.sort_values(by = 'Robust_Test_Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
